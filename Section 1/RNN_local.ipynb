{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIh5JCsU62y3"
      },
      "source": [
        "# Recurrent Neural Networks (RNNs) for Natural Language Processing (NLP) with Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vccEm97q9Dy"
      },
      "source": [
        "## Rationale\n",
        "\n",
        "Why do we need to consider a RNN architecture at all?\n",
        "1. CNNs processing windows of words are good at using some of the word proximities in sentences but fail to use the relationships that are present in longer sentences.\n",
        "2. Language uses sentences that create meaning as the sequences of words build meaning by being considered together over time as the sentence evolves. So **word order** matters.\n",
        "3. So we need to consider how to handle the construction of meaning **over time**.\n",
        "\n",
        "Consider the following two sentences -\n",
        "\n",
        "\n",
        "\n",
        "> The stolen car sped into the arena. \\\n",
        "The clown car sped into the arena.\n",
        "\n",
        "\n",
        "\n",
        "These are two almost identical sentences. However, by the time you the reader get to the end of each sentence, you will have formed a very different sense of what the different sentences mean.\n",
        "\n",
        "In order to somehow convey the contributon of the adjectives \"stolen\" or \"clown\" at the beginning of each sentence to the end of it's sentence, you need to incorporate some notion of **memory** so that their contributions to their sentences persist to construct the meanings at the end of the sentence.\n",
        "\n",
        "This notion of **memory** is what RNNs introduce.\n",
        "\n",
        "To extend your practical experience, this code is intended for running on your local machine and not particularly on Colab. While the *free* version of Colab provides a good environment to learn, it can be limited in the degree to which it facilitates experiments without running into resource constraints."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlaTnJaK68y4"
      },
      "source": [
        "## Import relevant libraries\n",
        "Import the relevant libraries and set up your **working directory** in the code below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9T1R-qk6jjs"
      },
      "outputs": [],
      "source": [
        "%pip install nltk\n",
        "%pip install gensim\n",
        "import glob # string manipulation for constructing directory paths\n",
        "import nltk # bring in the Natural Language Tool Kit\n",
        "import os # handle Operating System file tasks\n",
        "from random import shuffle # facility to generate random selections\n",
        "from nltk.tokenize import TreebankWordTokenizer # Tokenize the strings\n",
        "from gensim import models\n",
        "\n",
        "# Set your working directory in the code here\n",
        "os.chdir('C:/Users/patrick.denny/OneDrive - University of Limerick/Documents/AdvancedNLP/Module Material/3. and 4. CNNs and RNNs with Sentiment Analysis/Example Code')\n",
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFV-vApilflh"
      },
      "source": [
        "If not already installed, install **Tensorflow** and **Keras** using **pip** or **pip3**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIlu1o3Ilflh"
      },
      "outputs": [],
      "source": [
        "!pip3 install tensorflow\n",
        "!pip3 install keras\n",
        "# if you are still having trouble, then they might just need an update and that is straightforward\n",
        "# pip3 install tensorflow --upgrade\n",
        "# pip3 install keras --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNohfOyibtgu"
      },
      "source": [
        "## Get word vectors and training data\n",
        "This is the same data as you used in the previous CNN code. If you have already downloaded these data to your working directory, then you don't need to do so again. The data consist of\n",
        "- the aclImbd database of reviews and their scored sentiments, for training\n",
        "- word vectors trained on a Google News corpus, for converting tokens into word vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_CjF3cUlflh",
        "outputId": "06984cd9-6d1f-42ef-d069-69f28bdc3628"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "# download the IMDb database\n",
        "!wget 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
        "# download the Google News corpus\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpKaAtj4lfli"
      },
      "source": [
        "You will also need to download the rather large Google News corpus to your **working directory** ; it can be downloaded using this Kaggle page - https://www.kaggle.com/datasets/leadbest/googlenewsvectorsnegative300\n",
        "\n",
        "This allows us to create word vectors that are based on 20000 words from the Google News corpus. We can choose more or fewer words for tokenizing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGYj2h_Clfli"
      },
      "outputs": [],
      "source": [
        "w = models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', limit = 20000, binary=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFdlSJKu7zsY"
      },
      "source": [
        "## Data preprocessing\n",
        "As ever, there is a little bit of preprocessing to get the data into a shape we can use.\n",
        "\n",
        "### Load in samples and shuffle them together\n",
        "The idea here is that we\n",
        "- Read in the positive and negative sentiments\n",
        "- Associate their ground truth labels with them, i.e., whether a specific sentiment is actually positive or negative\n",
        "- Shuffle them to avoid bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUBgJ55j8Nbp"
      },
      "outputs": [],
      "source": [
        "def pre_process_data(filepath):\n",
        "  \"\"\"\n",
        "  Load pos and neg examples from separate dirs then shuffle them together.\n",
        "  \"\"\"\n",
        "  positive_path = os.path.join(filepath, 'pos')\n",
        "  negative_path = os.path.join(filepath, 'neg')\n",
        "  pos_label = 1\n",
        "  neg_label = 0\n",
        "  dataset = []\n",
        "  for filename in glob.glob(os.path.join(positive_path, '*.txt')):\n",
        "    with open(filename, 'r', encoding = \"utf-8\") as f:\n",
        "      dataset.append((pos_label, f.read()))\n",
        "  for filename in glob.glob(os.path.join(negative_path, '*.txt')):\n",
        "    with open(filename, 'r', encoding = \"utf-8\") as f:\n",
        "      dataset.append((neg_label, f.read()))\n",
        "  shuffle(dataset)\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCZJyv9i-sAm"
      },
      "source": [
        "### Data tokenizer + vectorizer\n",
        "The recurrent neural network needs to operate with vectors of numbers, so we need to operate on each sample of words from a dataset accordingly in essentially two steps\n",
        "- **Tokenize** the words in the input data set, in this case using the Treebank Word Tokenizer\n",
        "- Determine the corresponding **word vector** for the token.\n",
        "\n",
        "These are then collected and returned by the helper routine below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZQh9nKC-xWC"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_vectorize(dataset):\n",
        "    tokenizer = TreebankWordTokenizer() # The Treebank Tokenizer from the Natural Language Toolkit\n",
        "    vectorized_data = []\n",
        "    for sample in dataset:\n",
        "      tokens = tokenizer.tokenize(sample[1])\n",
        "      sample_vecs = []\n",
        "      for token in tokens:\n",
        "        try:\n",
        "          sample_vecs.append(w[token])\n",
        "        except KeyError:\n",
        "          pass # this is just if there is no matching token in the vocabulary\n",
        "      vectorized_data.append(sample_vecs)\n",
        "    return vectorized_data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-QLcy-0_sf9"
      },
      "source": [
        "### Target unzipper\n",
        "Peel off the target values (the **ground truth** of **positive** or **negative sentiment** for a given sample) from a dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZQsT-Su_vv_"
      },
      "outputs": [],
      "source": [
        "def collect_expected(dataset):\n",
        "    \"\"\" Peel off the target values from the dataset \"\"\"\n",
        "    expected = []\n",
        "    for sample in dataset:\n",
        "        expected.append(sample[0])\n",
        "    return expected"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5jrfNn2AI_z"
      },
      "source": [
        "### Load and prepare your data\n",
        "This is where we split the data into **training** and **testing** components with corresponding labels. We will, iteratively\n",
        "- **train** the network with training data and its corresponding labels\n",
        "- **test** the network performance with our test data and its corresponding labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_nUsVmoAQm8",
        "outputId": "ebd356ac-6345-4416-d560-7225451471ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "C:\\Users\\patrick.denny\\OneDrive - University of Limerick\\Documents\\AdvancedNLP\\Module Material\\3. and 4. CNNs and RNNs with Sentiment Analysis\\Example Code\n"
          ]
        }
      ],
      "source": [
        "print(os.getcwd())\n",
        "dataset = pre_process_data('aclImdb/train')\n",
        "vectorized_data = tokenize_and_vectorize(dataset)\n",
        "expected = collect_expected(dataset)\n",
        "split_point = int(len(vectorized_data) * .8) # Split the train and tests into an 80/20 unshuffled split\n",
        "x_train = vectorized_data[:split_point]\n",
        "y_train = expected[:split_point]\n",
        "x_test = vectorized_data[split_point:]\n",
        "y_test = expected[split_point:]\n",
        "\n",
        "# Clear unneeded data\n",
        "del(vectorized_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4iRHLlElflj"
      },
      "source": [
        "### What do the data look like?\n",
        "The data are essentially a **sentiment of a text** and its **corresponding text** and there are many of them..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6ylFWEQlflj",
        "outputId": "00f2eb40-9f32-471d-a64d-04d1473a332b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(0, '...from this awful movie! There are so many things wrong with this film, acting, writing, direction, editing, etc. that it\\'s amazing that something rises to the top and proves itself to be the absolute worst. The music! I noted that the film has two composers listed. This must be the reason why every single frame has music, of the absolute worst \"D\" movie style drivel. They have never heard of the expression \"less is more\". It got so painful to listen to, I muted the sound every time there was no dialogue, not that the dialogue was that good. You have to feel sorry for Robert Wagner and Tom Bosley, I\\'m sure they didn\\'t see roles like this in the twilight of their careers. See it at your own risk.')\n",
            "\n",
            " The number of entries in the dataset is  25000\n"
          ]
        }
      ],
      "source": [
        "print(dataset[10])\n",
        "print('\\n The number of entries in the dataset is ', len(dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4IwIsXwBE6P"
      },
      "source": [
        "## Building and Training our Recurrent Neural Network\n",
        "### Initialize the network parameters\n",
        "Now we set **hyperparameters** for the network prior to training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qWP8o12BLVz"
      },
      "outputs": [],
      "source": [
        "maxlen =  400   # The maximum length of an input sequence for training\n",
        "batch_size = 16 # The number of steps before a backpropagation is performed\n",
        "embedding_dims = 300 # The number of word vector embedding dimensions used\n",
        "epochs = 5 # The overall number of training iterations\n",
        "num_neurons = 50 # Number of neurons in the hidden layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O--7OWIdfpQn"
      },
      "source": [
        "### Tidying the input data\n",
        "Next we have to pad or truncate the sequence of tokens in each review so that we have a fixed input size for our RNN training input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ju7RHyBofxLO"
      },
      "outputs": [],
      "source": [
        "def pad_trunc(data, maximumlen):\n",
        "    \"\"\"\n",
        "    Pad or truncate each review to the size set by the hyperparameter maxlen\n",
        "    because we need each input to have consistently sized tokens.\n",
        "    \"\"\"\n",
        "    new_data = []\n",
        "\n",
        "    zero_vector = []\n",
        "    for _ in range(len(data[0][0])):\n",
        "        zero_vector.append(0.0)\n",
        "\n",
        "    for sample in data:\n",
        "        if len(sample) > maximumlen: # if the input is too large, truncate it\n",
        "            temp = sample[:maximumlen]\n",
        "        elif len(sample) < maximumlen: # if the input is too small, pad it\n",
        "            temp = sample\n",
        "            # Append the appropriate number zero vectors to the list\n",
        "            additional_elems = maximumlen - len(sample)\n",
        "            for _ in range(additional_elems):\n",
        "                temp.append(zero_vector)\n",
        "        else:\n",
        "            temp = sample\n",
        "        new_data.append(temp)\n",
        "    return new_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBDLzg0FBSKA"
      },
      "source": [
        "### Load your test and training data\n",
        "We go through a similar exercise as before for the CNN, by ensuring consistently sized tokens and by reshaping the training and test data into **NumPy array structures** as these are much easier for the system to manipulate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCOJWEmwBV9w"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Pad or truncate the inputs to the length maxlen\n",
        "x_train = pad_trunc(x_train, maxlen)\n",
        "x_test = pad_trunc(x_test, maxlen)\n",
        "\n",
        "x_train = np.reshape(x_train, (len(x_train), maxlen, embedding_dims))\n",
        "y_train = np.array(y_train)\n",
        "x_test = np.reshape(x_test, (len(x_test), maxlen, embedding_dims))\n",
        "y_test = np.array(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ztq5-pPlCFne"
      },
      "source": [
        "### Initialize an empty Keras network\n",
        "We start to build a sequential neural network using Keras. Firstly, we set up a Sequential model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pm24SvQOCJcw"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, SimpleRNN\n",
        "model = Sequential()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXGVwqimCUU-"
      },
      "source": [
        "### Add a Recurrent layer\n",
        "As it is a recurrent neural network, we need to add a simple recurrent neural network layer which will set up the appropriate \"plumbing\" :-)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRpHyr-oCcx7"
      },
      "outputs": [],
      "source": [
        "model.add(SimpleRNN(\n",
        "    num_neurons, return_sequences=True,\n",
        "    input_shape=(maxlen, embedding_dims)\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN23tpvmCprb"
      },
      "source": [
        "### Add a Dropout layer\n",
        "We want the network to be as small and efficient as possible and not be overtrained, so we add a dropout layer. The dropout layer is a countermeasure against the network taking on too many fine details in the training data that adversely affect its **generalisability**. The dropout layer just creates a layer the same size as the previous layer with **one-to-one** connections (i.e. each neuron in the droput layer is connected to exactly one in the previous layer), but randomly \"drops out\" 20 \\% of the connections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUYZMknNCsDa"
      },
      "outputs": [],
      "source": [
        "model.add(Dropout(0.2)) # was 0.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKqBDMEwlfll"
      },
      "source": [
        "### Add a Flatten layer\n",
        "This layer \"flattens\" the data and assigns a node in the layer to each of the sample data so far."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cyn7f8JPlfll"
      },
      "outputs": [],
      "source": [
        "model.add(Flatten())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csZhV1iUlfll"
      },
      "source": [
        "### Add a Dense layer\n",
        "The dense layer serves a very useful purpose. Thus far, the networking in the layer has been getting more and more intricate and quite large, but the dense layer \"condenses\" the information so far. It does this by giving a weighted combination of the neurons in the previous Flatten layer and apply a sigmoid activiation function to that whole layer. A **sigmoid activation function** compresses a whole range of numbers into a value between 0 and 1 and for this sentiment analysis application, where we represent our ground truth negative and postive values as between 0 and 1, this is ideal. Ultimately, it puts our **known labels** (sentiment **ground truth**) and our **predicted values** (our **predictions** from the network) in the same format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40Vc59Odlfll"
      },
      "outputs": [],
      "source": [
        "model.add(Dense(1, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfJ-phCwC6km"
      },
      "source": [
        "### Compile your recurrent network\n",
        "Lets stick the parts of the network together and see how many parameters we have."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "024pRqVqC_sZ"
      },
      "outputs": [],
      "source": [
        "model.compile('rmsprop','binary_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYiMHWXBlflm"
      },
      "source": [
        "You will note that the scale of the parameters in this RNN network is **an order of magnitude smaller** than its CNN counterpart.\n",
        "\n",
        "Now, lets take a look at what the size of each of these layers is.\n",
        "\n",
        "The **SimpleRNN** *shape* is based on\n",
        "* weights for each embedding dimension for each token that is used; the embedding dimension of the word vectors is **300**\n",
        "* a **bias-term** for when providing the output, as the next recurrence applied to the network data will do the normal network calculations that nodes perform, which involves using the **weighted** inputs and a **bias**. So the number of bias terms here is just **1**.\n",
        "* the weights corresponding to the hidden layer that are passed as **output** from the network at time **t** into the network as an **input** at time **t+1**, in this case **50**\n",
        "\n",
        "That gives us **300** + **1** + **50** = **351** parameters to consider at each time step.\n",
        "But we have in turn **50** neurons in the hidden layer, so each of these has to be considered too.\n",
        "\n",
        "This gives a grand total of **351** x **50** = **17,550** for the \"simple\" RNN setup.\n",
        "\n",
        "Taking the subsequent layers in turn :\n",
        "\n",
        "* The **Dropout layer** requires no parameters as it just creates a layer the same size as the previous layer with **one-to-one** connections, but randomly \"drops out\" 20 \\% of those connections. The **\"20 \\%\"** is not a **parameter** of the network that is updated during processing, but a **hyperparameter** that is used to set-up the network architecture.\n",
        "* Similarily, the **Flatten layer** has no associated parameters, as it is just spreading out all of the previous neurons (**400** x **50** = **20,000**)\n",
        "* The **Dense layer** has a weight for each of the neurons in the Flatten layer and a single bias term (**20,000** + **1** = **20,001**) but outputs a single value so the shape of the output is **1**.\n",
        "\n",
        "That gives a total of **17,550** + **20,001** = **37,551** parameters in the network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUKoJPy_DOSY"
      },
      "source": [
        "### Train and save your model\n",
        "I recommend you do this on a PC or workstation or some form of local computer instead of Colab and this will take time. I've tried this on Colab and sometimes it crashes the session because I am using the free version of Colab. Ideally, you will have a GPU on your computer; if not, the code will still run but will take longer. This is not much of an issue if you are running a network for educational purposes, but becomes very noticeable very quickly if you are doing serious experiments with large datasets.\n",
        "\n",
        "From a module learning perspective, it is more important that you have code that runs so that you can learn it rather than having high precision outcomes that require large amounts of time, data and processing overhead.\n",
        "\n",
        "If you can build and appreciate the smaller models, then graduating to the larger ones is really a matter of\n",
        "* exploiting that extra time, data and processing\n",
        "* re-running and challenging your data\n",
        "* trying out different selections of hyperparameter values\n",
        "* rinsing and repeating..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqOsPH8KDSeZ"
      },
      "outputs": [],
      "source": [
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))\n",
        "\n",
        "model_structure = model.to_json()\n",
        "with open(\"simplernn1.json\", \"w\") as json_file:\n",
        "  json_file.write(model_structure)\n",
        "model.save_weights(\"simplernn1.weights.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kH47mk0lfln"
      },
      "source": [
        "You can **reload** a **saved** model as follows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOhekHYilfln",
        "outputId": "f03e55c0-7a4d-4255-bf50-c886d0252256"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " simple_rnn_1 (SimpleRNN)    (None, 100, 50)           17550     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 100, 50)           0         \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 5000)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 5001      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 22,551\n",
            "Trainable params: 22,551\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Import the module to read in a JSON format model\n",
        "\n",
        "from keras.models import model_from_json\n",
        "\n",
        "# Now instantiate a model\n",
        "\n",
        "with open(\"simplernn_model1.json\", \"r\") as json_file:\n",
        "  json_string = json_file.read()\n",
        "model2 = model_from_json(json_string)\n",
        "\n",
        "# Once the model structure exists, set its characteristic weights\n",
        "\n",
        "model2.load_weights('simplernn1.weights.h5')\n",
        "model2.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHrILsVfEoSJ"
      },
      "source": [
        "### Run a simple test on your model\n",
        "Now, the fun bit, run the code and see what it does.\n",
        "Recall\n",
        "- the closer the predicted sentiment is to **1**, the more positive the sentiment\n",
        "- the closer the predicted sentiment is to **0**, the more negative the sentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U82c3w3SEtK6",
        "outputId": "8a411c8b-d2f4-4768-8b70-29f4f0cab301"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 720ms/step\n",
            "It is a beautiful day outside, the weather is wonderful and I am so happy to be alive! [[0.93817055]]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "This is one of the worst movies I have ever seen. It is rubbish. [[0.3374831]]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "I am exhausted and I just want to die. [[0.37458467]]\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "This is terrible.Terrible, terrible, terrible, terrible, terrible, terrible. How can anything be as bad as this? [[0.21551856]]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "I have great class and this makes me very happy. They are learning very interesting technology. [[0.8899865]]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "Wow! [[0.75701046]]\n"
          ]
        }
      ],
      "source": [
        "#\n",
        "# Create a couple of samples for prediction and predict\n",
        "#\n",
        "sample_1 =\"It is a beautiful day outside, the weather is wonderful and I am so happy to be alive!\"\n",
        "sample_2 =\"This is one of the worst movies I have ever seen. It is rubbish.\"\n",
        "sample_3 =\"I am exhausted and I just want to die.\"\n",
        "sample_4 =\"This is terrible.Terrible, terrible, terrible, terrible, terrible, terrible. How can anything be as bad as this?\"\n",
        "sample_5 =\"I have great class and this makes me very happy. They are learning very interesting technology.\"\n",
        "sample_6 = \"Wow!\"\n",
        "\n",
        "samples = [sample_1, sample_2, sample_3, sample_4, sample_5, sample_6]\n",
        "\n",
        "for sample in samples :\n",
        "  vec_list = tokenize_and_vectorize([(1, sample)])\n",
        "  test_vec_list = pad_trunc(vec_list, maxlen)\n",
        "  test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen, embedding_dims))\n",
        "  print(sample, model.predict(test_vec))\n",
        "\n",
        "\n",
        "# vec_list = tokenize_and_vectorize([(1, sample_4)])\n",
        "# test_vec_list = pad_trunc(vec_list, maxlen)\n",
        "# test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen, embedding_dims))\n",
        "# model.predict(test_vec)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rlZEPCEEzMl"
      },
      "source": [
        "## Build a larger network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iy9VhKcE2_c"
      },
      "source": [
        "The performance of network depends on a combination of choices of **architecture**, **training data** and **hyperparameters**. So, again, lets experiment with the number of neurons by setting **num_neurons = 100** and see what happens to the network performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eoJ2naeE-Qx",
        "outputId": "7ef0bcc6-a245-4f71-9351-f1f0f8fec06f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " simple_rnn_2 (SimpleRNN)    (None, 400, 100)          40100     \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 400, 100)          0         \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 40000)             0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 40001     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 80,101\n",
            "Trainable params: 80,101\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "num_neurons = 100\n",
        "model_bigger = Sequential()\n",
        "model_bigger.add(SimpleRNN(\n",
        "    num_neurons, return_sequences=True, input_shape=(maxlen, embedding_dims)\n",
        "))\n",
        "model_bigger.add(Dropout(.2))\n",
        "model_bigger.add(Flatten())\n",
        "model_bigger.add(Dense(1, activation='sigmoid'))\n",
        "model_bigger.compile('rmsprop', 'binary_crossentropy', metrics=['accuracy'])\n",
        "model_bigger.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MHtoKwSkmdb"
      },
      "source": [
        "### Train the larger network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTiu3tNhkp3C"
      },
      "outputs": [],
      "source": [
        "model_bigger.fit(x_train, y_train,\n",
        "          batch_size+batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_data=(x_test,y_test))\n",
        "\n",
        "# Save the trained network\n",
        "model_structure = model_bigger.to_json()\n",
        "with open(\"simplernn_model_bigger.json\", \"w\") as json_file_bigger:\n",
        "  json_file_bigger.write(model_structure)\n",
        "model_bigger.save_weights(\"simplernn_bigger.weights.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMAimcUplLrK"
      },
      "source": [
        "## Predicting sentiments\n",
        "Lets make sentiment predictions. Note that **the network has never seen these sentences before**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpQxHztClNnw",
        "outputId": "241705e0-2c7f-472d-a02f-c34f9341e7ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 106ms/step\n",
            "It is a beautiful day outside, the weather is wonderful and I am so happy to be alive! [[0.7948919]]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "This is one of the worst movies I have ever seen. It is rubbish. [[0.4005727]]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "I am exhausted and I just want to die. [[0.2393439]]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "This is terrible.Terrible, terrible, terrible, terrible, terrible, terrible. How can anything be as bad as this? [[0.06073603]]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "I have a great class and this makes me very happy. They are learning very interesting technology. [[0.8801557]]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Wow! [[0.43194225]]\n"
          ]
        }
      ],
      "source": [
        "#\n",
        "# Create a couple of samples for prediction and predict\n",
        "#\n",
        "sample_1 =\"It is a beautiful day outside, the weather is wonderful and I am so happy to be alive!\"\n",
        "sample_2 =\"This is one of the worst movies I have ever seen. It is rubbish.\"\n",
        "sample_3 =\"I am exhausted and I just want to die.\"\n",
        "sample_4 =\"This is terrible.Terrible, terrible, terrible, terrible, terrible, terrible. How can anything be as bad as this?\"\n",
        "sample_5 =\"I have a great class and this makes me very happy. They are learning very interesting technology.\"\n",
        "sample_6 = \"Wow!\"\n",
        "\n",
        "samples = [sample_1, sample_2, sample_3, sample_4, sample_5, sample_6]\n",
        "\n",
        "# If you HAVE created the model already, then read it back in to use it\n",
        "\n",
        "from keras.models import model_from_json\n",
        "with open(\"simplernn_model_bigger.json\", \"r\") as json_file_bigger:\n",
        "  json_string = json_file_bigger.read()\n",
        "model_bigger = model_from_json(json_string)\n",
        "model_bigger.load_weights('simplernn_bigger.weights.h5')\n",
        "\n",
        "# Here you are passing a dummy value into the first elemnt of the tuple because\n",
        "# your helper function expects it from the way it processed the initial data.\n",
        "# That value won't ever see the network, so it doesn't matter what it is\n",
        "# Could this and should this be syntactically better for production code? Yes.\n",
        "\n",
        "for sample in samples :\n",
        "  vec_list = tokenize_and_vectorize([(1, sample)])\n",
        "  test_vec_list = pad_trunc(vec_list, maxlen)\n",
        "  test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen, embedding_dims))\n",
        "  print(sample, model_bigger.predict(test_vec))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfd-_xeLlflp"
      },
      "source": [
        "What do you think of the scores? Why might they be what they are?\n",
        "\n",
        "Also, for experimentation\n",
        "- Consider different training corpora - https://www.kaggle.com/datasets?search=corpus\n",
        "- Compare the sizes of the networks, the runtimes of the predictions of the networks and the accuracy of the network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrCV9Koxo66W"
      },
      "source": [
        "## Two-way street - bidirectional RNNs for NLP\n",
        "Sometimes we get information about a key word in a sentence later in the sentence and as humans we can make sense of it. For example, consider the sentence\n",
        "\n",
        "> He taught the class of M.Sc. students\n",
        "\n",
        "As you read this sentence, you will see that someone taught a class and then you will see that the class was of M.Sc. students; you could associate **teaching** with **a class** and then discover that class was **M.Sc. students**. In principle you could read the words backwards and see that there **students**, that the students where doing an **M.Sc.** and that the M.Sc. students were in a **class** and that that class was being **taught**. So, you can make richer sequences of inferences and relatioships by going backwards through the sentence. It is possible to do this with RNNs too."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyzlVEunpJlp"
      },
      "source": [
        "### Build a **Bidirectional** RNN\n",
        "This is just some sample code for building a bidirectional RNN using keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJxqrJCKpYcD",
        "outputId": "694b4d87-d60c-4978-c3b7-85aba5386f20"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nfrom keras.models import Sequential\\nfrom keras.layers import SimpleRNN\\nfrom keras.layers.wrappers import Bidirectional\\n\\nnum_neurons = 10\\nmaxlen = 100\\nembedding_dims = 300\\n\\nmodel = Sequential()\\nmodel.add(Bidirectional(\\n    SimpleRNN(num_neurons, return_sequences+True), input_shape=(maxlen, embedding_dims))\\n)\\n'"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "from keras.models import Sequential\n",
        "from keras.layers import SimpleRNN\n",
        "from keras.layers.wrappers import Bidirectional\n",
        "\n",
        "num_neurons = 10\n",
        "maxlen = 100\n",
        "embedding_dims = 300\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Bidirectional(\n",
        "    SimpleRNN(num_neurons, return_sequences+True), input_shape=(maxlen, embedding_dims))\n",
        ")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phOnh7xflflp"
      },
      "source": [
        "# Summary\n",
        "Just after the **Dense** layer, a vector of shape **number of neurons x 1** comes out of the last step of the **Recurrent** layer and it is a sort of encoding of the sequence of input tokens. It is like the notion of **thought vector** that we discussed for CNNs and it is a powerful notion that we take into the last section of this module so that we can amplify its power.\n",
        "\n",
        "So, what have we found in summary?\n",
        "- In NLP sequences, and in sentences in general, the meaning of words is affected by previous words\n",
        "- Splitting a natural language statement into a time sequence of tokens can help get a deeper meaning from the sentences\n",
        "- You can backpropagate learning errors \"in time\" as well as in the normal way that we did with CNNs\n",
        "- RNNs are particularly deep, so they have gradients that can disappear or explode and this needs to be considered\n",
        "- Efficient modelling of natural language character sequences was impossible until RNNs were invented\n",
        "- Weights in an RNN are adjusted across time for a given sample, thereby capturing the meaning that is hidden in the sequencing of the words\n",
        "- There are different metrics, such as accuracy, that can be used to examine the output of RNNs\n",
        "- You can exploit the sequencing of tokens in an RNN in both directxions with bidirectional RNNs\n",
        "- The notion of a **thought vector** that somehow captures an underlying **meaning** of an NLP sequence appears again and is worth considering in the next section"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}