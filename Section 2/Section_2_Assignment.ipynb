{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Section 3 MN5002 (LSTMs, Language Models, seq2seq architectures) - Assignment\n",
        "Firstly, modify and run the code below using the [Alice In Wonderland text](https://www.gutenberg.org/cache/epub/11/pg11.txt) and consider the output in the context of the review remarks in the section **Main Exercise** below.\n",
        "\n",
        "Then pick yourself a text from [Project Gutenberg's texts](https://www.gutenberg.org/ebooks) and the format you should download is **Plain Text UTF-8** ; when you are considering a text, ensure you are happy that the corpus is large enough, so choose an author with a lot of text. Have a good explore around the texts of Project Gutenberg there is much there.\n",
        "\n",
        "Using your chosen text(s), create an LSTM RNN based Language Model with Keras.\n",
        "Make improvements to the language model using what you learned from the example notebook. Show text examples of outputs critiquing them and implementing modifications to address issues you see in the generated text.\n",
        "\n",
        "Also, post comments in the Section 3 Discussion section.\n",
        "Make sure that your outputs are on in your notebooks so that when the code is submitted the outputs are clear and it will not be necessary to run the code to see the outcomes.\n",
        "\n",
        "### More tips\n",
        "\n",
        "It is strongly recommended that this is run with **a Colab GPU runtime** using **Runtime -> Change runtime type** and selecting GPU. You will need to store your model files and data on your Google Drive if you do not want to lose your work between runtimes and you will need to mount your Google Drive each time you run this code so that you can access your data. The base model took me about 7 minutes to train using a T4 Colab GPU runtime."
      ],
      "metadata": {
        "id": "OSb49iJSxY7U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Automatic text generation with LSTM RNNs in Python with Keras\n",
        "Generate text that mimics an author's style. The approach below is based on [code by Jason Brownless](https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/)"
      ],
      "metadata": {
        "id": "VB84cwhueqFA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "K2OKlA4QegcD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import os\n",
        "import sys\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.utils import to_categorical\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyDHrcGEHgyl",
        "outputId": "5d22a796-8da3-491e-a670-b7e39795f6ed"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Call up an ASCII text file and convert the characters to lowercase. This is an approach called **case folding** and it reduces the number of tokens under consideration."
      ],
      "metadata": {
        "id": "TG56KB50e8ti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load ascii text and covert to lowercase\n",
        "os.chdir('/content/drive/MyDrive/MSc NLP Files')\n",
        "filename = \"AliceInWonderland.txt\"\n",
        "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
        "raw_text = raw_text.lower()"
      ],
      "metadata": {
        "id": "N1cWYs9tfL7L"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare the data for modelling in by a neural network. This is going to be a character based representation, so we can associate each character with a unique integer."
      ],
      "metadata": {
        "id": "ufVU0VH5fkW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create mapping of unique chars to integers\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))"
      ],
      "metadata": {
        "id": "O7SLj1CCf47K"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, get a quick summary of what we have so far."
      ],
      "metadata": {
        "id": "RhSpbeJwf7nS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Characters: \", n_chars)\n",
        "print(\"Total Vocab: \", n_vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KogFb2NtgA9R",
        "outputId": "a9f08042-6e6c-4bb9-ab34-ad21c60e21d3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Characters:  163917\n",
            "Total Vocab:  64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If there are more than 26 characters, it means that there are several other types of character there - this may or may not add to the meaning. If you remove the extra characters, you may be increasing the capacity to extract meaning, or you may be increasing the \"noise\" and this is a source of experiment. (Watch out for this in your generated texts later)."
      ],
      "metadata": {
        "id": "RAfNeiuSgFBF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up the generation of text\n",
        "Text generation is,under the hood, very similar to the normal classification tasks that you will have seen before. Ordinary training for classification consists of a set of training data, where each training instance has a label.\n",
        "\n",
        "In text generation, the training instance is **a sequence of seq_length characters** and the label is **the (seq_length + 1) th character** ! This way, the network picks up the way the text is \"steered\" when an author writes."
      ],
      "metadata": {
        "id": "pcjWr_Uqg0G4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "\tseq_in = raw_text[i:i + seq_length]\n",
        "\tseq_out = raw_text[i + seq_length]\n",
        "\tdataX.append([char_to_int[char] for char in seq_in])\n",
        "\tdataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print(\"Total Patterns: \", n_patterns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9OGBNfdhwNu",
        "outputId": "1eb35064-a3ce-4dc7-a1dd-60d8168d7ae1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Patterns:  163817\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the training data are in place, you must do the following\n",
        "* transform the list of input sequences into the form *{samples, times steps, features}* expected by an LSTM network\n",
        "* Rescale the integers to the range 0-to-1 to make the patterns easier to learn using the sigmoid function\n",
        "* Finally, in order to get back from encoding to text data, you need to convert the ouput pattern (single character \"classifications\" as integers) into a one-hot encoding, so that you can determine which of the characters in the vocabulary is being indicated."
      ],
      "metadata": {
        "id": "HQy4wP5Qh1E9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reshape X to be [samples, time steps, features]\n",
        "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = to_categorical(dataY)"
      ],
      "metadata": {
        "id": "vP4FB4_QjZKr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, define the LSTM model"
      ],
      "metadata": {
        "id": "h3ADn1_Sje9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "metadata": {
        "id": "jAt_O0INjiZM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88d053a6-0a74-4704-ff72-7d9aae1df142"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that there is **no dropout layer** and **no test set**. We are trying to maximize the expression of the language structure for judgement by a human being and the outputs will rarely be perfect expectations in the training data, so the usual metrics of accuracy will not be helpful.\n",
        "\n",
        "Also, as the modelling is slow, use model checkpointing to record all the network weights and select for the one with the **lowest loss**. The loss value will be built into the checkpoint filename, so this is very useful."
      ],
      "metadata": {
        "id": "9OVgxfGljohL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the checkpoint\n",
        "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.keras\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "metadata": {
        "id": "c5PWzGrQlDys"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now fit the model"
      ],
      "metadata": {
        "id": "Lx1TtciRlHbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TOtjtZxlI5L",
        "outputId": "bfb2dcfd-1db5-4f01-a8b7-9558593b2b4b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.1155\n",
            "Epoch 1: loss improved from inf to 3.03075, saving model to weights-improvement-01-3.0307.keras\n",
            "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 14ms/step - loss: 3.1154\n",
            "Epoch 2/20\n",
            "\u001b[1m1278/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 2.8737\n",
            "Epoch 2: loss improved from 3.03075 to 2.85104, saving model to weights-improvement-02-2.8510.keras\n",
            "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 14ms/step - loss: 2.8736\n",
            "Epoch 3/20\n",
            "\u001b[1m1279/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.7826\n",
            "Epoch 3: loss improved from 2.85104 to 2.76491, saving model to weights-improvement-03-2.7649.keras\n",
            "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 14ms/step - loss: 2.7826\n",
            "Epoch 4/20\n",
            "\u001b[1m1278/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.7183\n",
            "Epoch 4: loss improved from 2.76491 to 2.70088, saving model to weights-improvement-04-2.7009.keras\n",
            "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 14ms/step - loss: 2.7182\n",
            "Epoch 5/20\n",
            "\u001b[1m1279/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.6563\n",
            "Epoch 5: loss improved from 2.70088 to 2.64188, saving model to weights-improvement-05-2.6419.keras\n",
            "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 14ms/step - loss: 2.6562\n",
            "Epoch 6/20\n",
            "\u001b[1m1277/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.5950\n",
            "Epoch 6: loss improved from 2.64188 to 2.58468, saving model to weights-improvement-06-2.5847.keras\n",
            "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 14ms/step - loss: 2.5949\n",
            "Epoch 7/20\n",
            "\u001b[1m1278/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.5417\n",
            "Epoch 7: loss improved from 2.58468 to 2.53313, saving model to weights-improvement-07-2.5331.keras\n",
            "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 14ms/step - loss: 2.5417\n",
            "Epoch 8/20\n",
            "\u001b[1m1277/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.4914\n",
            "Epoch 8: loss improved from 2.53313 to 2.48569, saving model to weights-improvement-08-2.4857.keras\n",
            "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 14ms/step - loss: 2.4914\n",
            "Epoch 9/20\n",
            "\u001b[1m1278/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.4424\n",
            "Epoch 9: loss improved from 2.48569 to 2.44243, saving model to weights-improvement-09-2.4424.keras\n",
            "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 14ms/step - loss: 2.4424\n",
            "Epoch 10/20\n",
            "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.4040\n",
            "Epoch 10: loss improved from 2.44243 to 2.40287, saving model to weights-improvement-10-2.4029.keras\n",
            "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 14ms/step - loss: 2.4040\n",
            "Epoch 11/20\n",
            "\u001b[1m1279/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.3701\n",
            "Epoch 11: loss improved from 2.40287 to 2.36423, saving model to weights-improvement-11-2.3642.keras\n",
            "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 14ms/step - loss: 2.3701\n",
            "Epoch 12/20\n",
            "\u001b[1m1278/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.3251\n",
            "Epoch 12: loss improved from 2.36423 to 2.32714, saving model to weights-improvement-12-2.3271.keras\n",
            "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 14ms/step - loss: 2.3251\n",
            "Epoch 13/20\n",
            "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.2902\n",
            "Epoch 13: loss improved from 2.32714 to 2.29438, saving model to weights-improvement-13-2.2944.keras\n",
            "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 14ms/step - loss: 2.2902\n",
            "Epoch 14/20\n",
            "\u001b[1m1279/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.2599\n",
            "Epoch 14: loss improved from 2.29438 to 2.25980, saving model to weights-improvement-14-2.2598.keras\n",
            "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 14ms/step - loss: 2.2599\n",
            "Epoch 15/20\n",
            "\u001b[1m1279/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.2264\n",
            "Epoch 15: loss improved from 2.25980 to 2.22837, saving model to weights-improvement-15-2.2284.keras\n",
            "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 14ms/step - loss: 2.2264\n",
            "Epoch 16/20\n",
            "\u001b[1m1279/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.1931\n",
            "Epoch 16: loss improved from 2.22837 to 2.20094, saving model to weights-improvement-16-2.2009.keras\n",
            "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 14ms/step - loss: 2.1931\n",
            "Epoch 17/20\n",
            "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.1638\n",
            "Epoch 17: loss improved from 2.20094 to 2.16983, saving model to weights-improvement-17-2.1698.keras\n",
            "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 14ms/step - loss: 2.1638\n",
            "Epoch 18/20\n",
            "\u001b[1m1278/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.1389\n",
            "Epoch 18: loss improved from 2.16983 to 2.14230, saving model to weights-improvement-18-2.1423.keras\n",
            "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 14ms/step - loss: 2.1389\n",
            "Epoch 19/20\n",
            "\u001b[1m1279/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.1144\n",
            "Epoch 19: loss improved from 2.14230 to 2.11781, saving model to weights-improvement-19-2.1178.keras\n",
            "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 14ms/step - loss: 2.1144\n",
            "Epoch 20/20\n",
            "\u001b[1m1278/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.0842\n",
            "Epoch 20: loss improved from 2.11781 to 2.09047, saving model to weights-improvement-20-2.0905.keras\n",
            "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 14ms/step - loss: 2.0842\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7ad69385a650>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating Text with an LSTM network\n",
        "Firstly, load the data and define the network using the checkpoint file."
      ],
      "metadata": {
        "id": "_YZE1pvSlO3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the network weights\n",
        "filename = \"weights-improvement-20-2.0905.keras\" # Use the checkpoint file with the lowest weight here\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "metadata": {
        "id": "bpgYqZn_l7Y1"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You need to make a reverse mapping that to convert integers back to characters so that you can understand the predictions.\n",
        "\n",
        "Recall -\n",
        "* **Encoders** turn language tokens into numerical representations whereas\n",
        "* **Decoders** turn numerical representations back into language tokens\n",
        "* **Discriminative models** use operations on encoded data to perform classification tasks (e.g., sentiment analysis, named entities, etc.,) but\n",
        "* **Generative models** are used to create outputs that have the same form as the inputs (e.g., word tokens as input to create word tokens as output)"
      ],
      "metadata": {
        "id": "LIeM222tmGdo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "int_to_char = dict((i, c) for i, c in enumerate(chars))"
      ],
      "metadata": {
        "id": "7nvnmPpIm-Fd"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, make predictions; start with a seed sequence as input, generate the next character, then update the seed sequence to add the generated character on the end and trim off the first character (this is kind of generatively \"shuffling\" along a generated character sequence) and this goes on for as long as you want to generate characters (1000 characters in the example below)"
      ],
      "metadata": {
        "id": "FWUuKLl6nCA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pick a random seed\n",
        "start = np.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print(\"Seed:\")\n",
        "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "# generate characters\n",
        "for i in range(1000):\n",
        "\tx = np.reshape(pattern, (1, len(pattern), 1))\n",
        "\tx = x / float(n_vocab)\n",
        "\tprediction = model.predict(x, verbose=0)\n",
        "\tindex = np.argmax(prediction)\n",
        "\tresult = int_to_char[index]\n",
        "\tseq_in = [int_to_char[value] for value in pattern]\n",
        "\tsys.stdout.write(result)\n",
        "\tpattern.append(index)\n",
        "\tpattern = pattern[1:len(pattern)]\n",
        "print(\"\\nDone.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7JxXCL0nt64",
        "outputId": "a54deb49-f3d8-4025-ec21-45ea79035021"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed:\n",
            "\" ant state of change. if you are outside the united states,\n",
            "check the laws of your country in additio \"\n",
            "n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main exercise\n",
        "Consider the output that you have generated and then apply those considerations to a corpus that you pick yourself from [Project Gutenberg's texts](https://www.gutenberg.org/ebooks) and the format you should download is **Plain Text UTF-8** ; when you are considering an author, ensure you are happy that the corpus is large enough, so choose an author with a lot of text.\n",
        "\n",
        "* How sensible does the output look?\n",
        "* If it doesn't look sensible, what is messing it up?\n",
        "* Have you enough input data, i.e., is your input text file too small?\n",
        "* Are there characters in the sequence that should be systematically removed?\n",
        "* Is your **case folding strategy** appropriate?\n",
        "* Could you boost the information in your corpus by adding documents from the same author or from a similar author?\n",
        "* Predict fewer than 1,000 characters as output for a given seed?\n",
        "* Remove some/all punctuation from the source text and, therefore, from the models’ vocabulary\n",
        "* Train the model on padded sentences rather than random sequences of characters\n",
        "* Increase the number of training epochs to 100 or many hundreds\n",
        "* Add dropout to the visible input layer and consider tuning the dropout percentage\n",
        "* Tune the batch size; try a batch size of 1 as a (very slow) baseline and larger sizes from there\n",
        "* Add more memory units to the layers and/or more layers\n",
        "*Experiment with scale factors (**temperature**) when interpreting the prediction probabilities\n",
        "Change the LSTM layers to be “**stateful**” to maintain state across batches\n",
        "\n",
        "There are [several pieces of advice and experience from others in the Responses section](https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/) for the original code, make use of this. There is also a further example provided on the same page using a larger network that also uses Dropout, which I do not personally think is a good strategy in this case, but again, make use of what is available."
      ],
      "metadata": {
        "id": "lZVBUQULr9OW"
      }
    }
  ]
}