{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49SHTWzU6Wzl"
      },
      "source": [
        "# Automatic Text Generation with Deep Learning (Python and Keras)\n",
        "\n",
        "In this section we look at **automatic text generation** application using an LSTM architecture. This leads us to consider two different ways that we can look at deep learning methods in NLP applications, which we discuss here."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCIKTBz--yol",
        "outputId": "29ca7fef-e5b4-41e1-eadb-a57bef850fdf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBWSvbzG6Wzm"
      },
      "source": [
        "## Generative vs Discriminative models\n",
        "The word \"generation\" in **automatic text generation** connotes one of two classes of models that we use in NLP Deep Learning applications.\n",
        "\n",
        "> **Discriminative** models\n",
        ">> Models that **discriminate** between different classes of data.\n",
        ">> * The networks used in **Sentiment Analysis** can discriminate (classify) between positive and negative sentiments in data.\n",
        "\n",
        "> **[Generative](https://developers.google.com/machine-learning/gan/generative)** models\n",
        ">> Models that can **generate** new data instances.\n",
        ">> * The networks used in **Text Generation** can generate output text based on input text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22TT5lXC6Wzm"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Text corpus can be downloaded from here for example, with the complete works of Shakespeare\n",
        "# https://www.gutenberg.org/ebooks/100\n",
        "# Probably best to go for the Plain Text UTF-8 version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5UJ8trR6Wzm"
      },
      "source": [
        "## Library installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKGQMcuB6Wzn",
        "outputId": "45e5a117-2164-4015-c974-72455c54e051"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-gpu\n",
            "  Downloading tensorflow-gpu-2.12.0.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement glob (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for glob\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement os (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for os\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement random (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for random\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9656 sha256=f4d5e222acb6162b291a01d47068a82b2534f3a5457133691d4f3fb9dc9ccf84\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/b3/0f/a40dbd1c6861731779f62cc4babcb234387e11d697df70ee97\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (3.8.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras) (0.14.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "# If you have a GPU installed you may need to include a GPU version of Tensorflow\n",
        "!pip install --ignore-installed --upgrade tensorflow-gpu\n",
        "\n",
        "# Install relevant packages if not already installed\n",
        "!pip install glob\n",
        "!pip install nltk\n",
        "!pip install numpy\n",
        "!pip install os\n",
        "!pip install random\n",
        "!pip install gensim\n",
        "!pip install wget\n",
        "!pip install tensorflow\n",
        "!pip install keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "p9T1R-qk6jjs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d3ac02c-bb74-4003-c2f5-61b7a696bb37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/MN5002_Section3\n"
          ]
        }
      ],
      "source": [
        "import glob # string manipulation for constructing directory paths\n",
        "import nltk # bring in the Natural Language Tool Kit\n",
        "import os # handle Operating System file tasks\n",
        "from matplotlib import pyplot as plt # plot images\n",
        "import numpy as np\n",
        "from os.path import exists\n",
        "from random import shuffle # facility to generate random selections\n",
        "from nltk.tokenize import TreebankWordTokenizer # Tokenize the strings\n",
        "from gensim import models\n",
        "import wget\n",
        "\n",
        "# Set your working directory in the code here\n",
        "working_directory = \"/content/drive/MyDrive/MN5002_Section3\"\n",
        "os.chdir(working_directory)\n",
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHInzP8d6Wzo"
      },
      "source": [
        "So, lets look at **The Complete Works of Shakespeare** and use it to make a **character based language model** to generate text automatically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GmCnP1F36Wzo"
      },
      "outputs": [],
      "source": [
        "text = ''\n",
        "CompleteWorksOfShakespeareFile = './pg100.txt'\n",
        "\n",
        "if exists(CompleteWorksOfShakespeareFile):\n",
        "    with open(CompleteWorksOfShakespeareFile, encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ke6v6mrL6Wzo",
        "outputId": "ca0a1247-2731-4f0f-ebac-f4f89910a4e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "﻿The Project Gutenberg eBook of The Complete Works of William Shakespeare, by William Shakespeare\n",
            "\n",
            "This eBook is for the use of anyone anywhere in the United States and\n",
            "most other parts of the world at no cost and with almost no restrictions\n",
            "whatsoever. You may copy it, give it away or re-use it under the terms\n",
            "of the Project Gutenberg License included with this eBook or online at\n",
            "www.gutenberg.org. If you are not located in the United States, you\n",
            "will have to check the laws of the country where\n"
          ]
        }
      ],
      "source": [
        "print(text[:500])\n",
        "text = text.lower()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRSbDNwW6Wzo"
      },
      "source": [
        "## Building an LSTM language model to generate text\n",
        "Now lets build a language model to generate text.\n",
        "Lets look at this in terms of what we have always done so far when building a neural network\n",
        "- Provide **data**\n",
        "- Provide the **label** associated with data\n",
        "\n",
        "Now recall what we are trying to do with a language model; we are trying to get the system to generate the next word or character given a sequence of words or characters.\n",
        "\n",
        "So we can look at our data in a way that we had for CNNs and RNNs, which is actually fairly simple\n",
        "- Provide **data**\n",
        " - A sequence of **40 consecutive character tokens**\n",
        "- Provide the **label** associated with the data\n",
        " - The **41st character token** (i.e., the next character in the sequence)\n",
        "\n",
        "We will use **The Complete Works of Shakespeare** to generate a data set to train our language model.\n",
        "\n",
        "Furthermore, the training set will consist of **semi-redundant sequences**; Take 40 characters from the beginning of the text, move to the 3rd character from the beginning, take a sequence of 40 from there, move to the 6th character from the beginning, take a sequence of 40 from there. This is a form of **data augmentation**, creating an extended data set with valid label characteristics from the data.\n",
        "\n",
        "So what we are doing here is fundamentally no different to\n",
        "* coming up with an augmented training set of **data** and **labels** and\n",
        "* using them to **train a neural network** with an LSTM architecture.\n",
        "\n",
        "All that remains is to :\n",
        "- Set up the **hyperparameters** for building a LSTM neural network with **Keras**\n",
        "- Represent the tokens in a mathematical form that can be used by the neural network (we'll **one-hot encode** them)\n",
        "- construct the **character-based** LSTM network; again Keras does that cleanly\n",
        "- train the network\n",
        "- generate text using the network (the fun bit)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJvanxyd6Wzo"
      },
      "source": [
        "## Set up hyperparameters for building a LSTM neural network\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGi7jESJ6Wzo",
        "outputId": "bc47188d-9381-449f-c2eb-19a0cda4b8b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sequences: 1845146\n"
          ]
        }
      ],
      "source": [
        "maxlen = 40\n",
        "step = 3 # This is the stepsize in creating the semi-redundant sequences\n",
        "sentences = [] # These are the \"data\" we referred to above\n",
        "next_chars = [] # These are the \"labels\" we referred to above\n",
        "for i in range(0, len(text) - maxlen, step):\n",
        "    sentences.append(text[i: i + maxlen])\n",
        "    next_chars.append(text[i + maxlen])\n",
        "\n",
        "# How many sequences have we now?\n",
        "\n",
        "print('Number of sequences:', len(sentences))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYduTvY-6Wzp"
      },
      "source": [
        "## Represent the words by one-hot encoding\n",
        "We are going to make a dictionary of characters to **one-hot encoding index** and a **reverse dictionary** which allow us to go back from a one-hot encoding to its character token.\n",
        "\n",
        "This is just a matter of collecting and indexing the characters from corpus text we read in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xNdn9Kf86Wzp"
      },
      "outputs": [],
      "source": [
        "chars = sorted(list(set(text)))\n",
        "char_indices = dict((c,i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i,c) for i, c in enumerate(chars))\n",
        "\n",
        "# One-hot encoding provides one entry of 1 and all other entries are 0, so we can use a compact boolean data type\n",
        "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=bool)\n",
        "y = np. zeros((len(sentences), len(chars)), dtype=bool)\n",
        "\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        X[i, t, char_indices[char]] = 1\n",
        "    y[i, char_indices[next_chars[i]]] = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoSfd9L-6Wzp"
      },
      "source": [
        "## Construct the character-based LSTM network\n",
        "Let's construct the generative network and look at our considerations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "dsPWK0pm6Wzp",
        "outputId": "4e16c91a-ab8c-43b4-ed4e-c4ad745a5586"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │         \u001b[38;5;34m106,496\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m79\u001b[0m)                  │          \u001b[38;5;34m10,191\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_1 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m79\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">106,496</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">79</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">10,191</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">79</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m116,687\u001b[0m (455.81 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">116,687</span> (455.81 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m116,687\u001b[0m (455.81 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">116,687</span> (455.81 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.layers import LSTM\n",
        "from keras.optimizers import RMSprop\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape = (maxlen, len(chars))))\n",
        "model.add(Dense(len(chars)))\n",
        "model.add(Activation('softmax'))\n",
        "optimizer = RMSprop(learning_rate=0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tL7yxfs6Wzp"
      },
      "source": [
        "### More neurons in hidden layer\n",
        "When we did **sentiment analysis** we used **50** neurons in the hidden layer, but in our generative model we are trying to model much more complex behaviour, so we use **128**.\n",
        "\n",
        "### Categorical Crossentropy loss function\n",
        "In sentiment analysis, when we had only **two** categories (\"positive\" and \"negative\"), we could use **binary**_crossentropy as our loss function.\n",
        "Now that we have a variety of categories, basically the number of possible types of token that might come after a sequence, we use **categorical_crossentropy** that can update the loss function across this wider range of tokens.\n",
        "\n",
        "### Learning Optimizer - RMSProp\n",
        "Generally optimizers are used to introduce \"tricks\" into the learning process which provide faster learning or better accuracy. [RMSProp](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf) is one of these and works by \"updating each weight by adjusting the learning rate with a \"running average of the magnitudes of recent gradients for that weight\".\n",
        "The best way to create models is to use the experience of other in setting up the model and then exploring the hyperparameters and tricks that can be used in the model to improve its performance.\n",
        "\n",
        "### No Dropout Layer\n",
        "Also, you will notice that **there is no Dropout layer**. We are trying to learn as much as we can about the structure of training data, which uses **all the available input data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mil2Dqx56Wzp"
      },
      "source": [
        "## Train the network and assess\n",
        "This is the \"run the code and get a coffee, or go away and come back tomorrow\" part of deep learning and is pervasive among those developing such systems, so be patient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t56lCY5c6Wzp",
        "outputId": "a72f2820-b82d-4e66-9fdc-85210dd0cac7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m14416/14416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 4ms/step - loss: 1.8564\n"
          ]
        }
      ],
      "source": [
        "epochs = 1\n",
        "batch_size = 128\n",
        "model_structure = model.to_json()\n",
        "with open(\"shakes_lstm_model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_structure)\n",
        "for i in range(1):\n",
        "    model.fit(X,y,\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs)\n",
        "    model.save_weights(\"shakes_lstm_{}.weights.h5\".format(i+1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SaM6USX6Wzq"
      },
      "source": [
        "Looking at the above outcomes, the lowest losses come after the first pass, so lets load the associated saved model files.\n",
        "The real teaching role here is just to provide you with a bit of code to pull in a preferred model. You may decide to go back and change hyperparameters, change data, change optimizer, etc., and usually engineers try to do these in parallel to understand what might work best."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "l95sP1wZ6Wzq"
      },
      "outputs": [],
      "source": [
        "from keras.models import model_from_json\n",
        "\n",
        "with open(\"shakes_lstm_model.json\", \"r\") as json_file:\n",
        "  json_string = json_file.read()\n",
        "model = model_from_json(json_string)\n",
        "\n",
        "# Once the model structure exists, set its characteristic weights\n",
        "\n",
        "model.load_weights('shakes_lstm_1.weights.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oENFlQZH6Wzq"
      },
      "source": [
        "## Generate text using the network\n",
        "\n",
        "Now, we start to generate language from our network with some helper functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0k7PK9Y6Wzq"
      },
      "source": [
        "### Make a sampler to generate character sequences\n",
        "\n",
        "As the last layer of the network is a **softmax** function, the output vector will be a probability distribution over all possible outputs of the network. By looking at the highest value in the output vector, you can see what the network thinks has the highest probability of being the next character.\n",
        "\n",
        "In terms of Python, this just means that the index of the output vector with the highest values (a number between 0 and 1) correlates with the index of the one-hot encoding of the expected token.\n",
        "\n",
        "But we don't want to go for the most probable character every time as the network would then be very boring and would not exercise its \"thoughts\".\n",
        "So we use a variable called \"**temperature**\" which will be used to determine how strictly or freely the next character is chosen; this provides the generative model with a **diversity** of possible outputs.\n",
        "\n",
        "Dividing the log by the temperature sharpens (temperature < 1) or squashes (temperature > 1) the probabilty distribution that reflects the learning.\n",
        "\n",
        "So \"Temperatures\" **less than 1** will try harder to reproduce the original text, but temperatures **greater than 1** give more freedom but squash out what was learned so the outputs tend towards jibberish."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ebvv91pc6Wzq"
      },
      "outputs": [],
      "source": [
        "# Make sampler to generate character sequences\n",
        "import random\n",
        "def sample(preds, temperature = 1.0):\n",
        "    preds = np.array(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEJTPiKf6Wzq"
      },
      "source": [
        "### Generation of diverse Shakespearean texts\n",
        "Now lets generate **3 texts** with **3 diversity levels**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXfrvxQF6Wzq",
        "outputId": "92e0cd81-b5f6-453d-f391-86cc58dc57b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"en he fawns, he bites; and when he bites\"\n",
            "t the peace of the prove\n",
            "the senter and the death and the poor brother.\n",
            "                                                                                                                                                                                                                                                                                                                                        \n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"en he fawns, he bites; and when he bites\"\n",
            "s it for no part,\n",
            "                                                                                                                                                                                                                                                                                                                                                                                              \n",
            "\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"en he fawns, he bites; and when he bites\"\n",
            "t kill well,\n",
            "there i see the tround so bont;\n",
            "to intiking, that have i for mennels, books, mistress?\n",
            "\n",
            "bardolph.\n",
            "o violetors, he wear upon you! oft, walbo5,\n",
            "i do i auge to so others alock you have duther\n",
            "that is other londer.\n",
            "\n",
            " [_exit discrood.— lang from my lady poor lett iep, lady,\n",
            "    did my talk your fasting nobled loves.\n",
            "  to so on the bacgush. was the country than i had could the lettes;\n",
            "thee \n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "start_index = random.randint(0, len(text) - maxlen -1)\n",
        "for diversity in [0.2, 0.5, 1.0]:\n",
        "    print()\n",
        "    print('----- diversity:', diversity)\n",
        "    generated = ''\n",
        "    sentence = text[start_index: start_index + maxlen]\n",
        "    generated += sentence\n",
        "    print('----- Generating with seed: \"' + sentence + '\"')\n",
        "    for i in range(400):\n",
        "        x = np.zeros((1, maxlen, len(chars)))\n",
        "        # Seed the trained network and see what it spits out as the next character\n",
        "        for t, char in enumerate(sentence):\n",
        "            x[0, t, char_indices[char]] = 1\n",
        "        # model makes a prediction\n",
        "        preds = model.predict(x, verbose=0)[0]\n",
        "        next_index = sample(preds, diversity)\n",
        "        # Look up which character that index represents (reverse dictionary)\n",
        "        next_char = indices_char[next_index]\n",
        "        generated += next_char\n",
        "        # Add the \"seed\" and drop the first character to keep the length the same\n",
        "        # This is now the seed for the next pass\n",
        "        sentence = sentence[1:] + next_char\n",
        "        sys.stdout.write(next_char)\n",
        "        sys.stdout.flush()\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lq_3Hbs96Wzr"
      },
      "source": [
        "Diversity 0.2 and 0.5 look a bit Shakespearean and look like flowing language but you can see that the Diversity 1.0 has lost a lot of learning and has tended towards jibberish."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXb6tJrd6Wzr"
      },
      "source": [
        "### Improving the automatic text generator\n",
        "You can improve generative models if you want them more than just for fun, and there are steps that can be taken that include the following, suggested in Chapter 9 of [\"Natural Language Processing in Action\", first edition, Lane et.al.](https://www.manning.com/books/natural-language-processing-in-action),:\n",
        "* Expand the quantity and quality of the corpus\n",
        "* Expand the complexity of the model (number of neurons)\n",
        "* Implement a more refined case folding algorithm\n",
        "* Segment sentences differently\n",
        "* Add filters on grammar, spelling and tone to match your needs\n",
        "* Generate many more examples than you actually show your users....\n",
        "* Use see texts chosen from the context of the session to steer the chatbot towards useful topics\n",
        "* Use multiple different seed texts within each dialog round to explore what the chatbot can talk about and what the user finds helpful"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "b68f8234f4664cd92394ba08fd2bcacafca488f50355ff1b3e37ec2c2f0f7027"
      }
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}